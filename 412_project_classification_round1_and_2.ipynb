{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gzip\n",
    "import json\n",
    "\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Turkish StopWords\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "nltk.download('stopwords')\n",
    "turkish_stopwords = stopwords.words('turkish')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "train_classification_df = pd.read_csv(\"train-classification.csv\",)\n",
    "train_classification_df = train_classification_df.rename(columns={'Unnamed: 0': 'user_id', 'label': 'category'})\n",
    "\n",
    "# Unifying labels\n",
    "train_classification_df[\"category\"] = train_classification_df[\"category\"].apply(str.lower)\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "# Fit and transform the 'category' column\n",
    "train_classification_df[\"encoded_category\"] = label_encoder.fit_transform(train_classification_df[\"category\"])\n",
    "\n",
    "# Create mapping of usernames to unified (lowercase) categories\n",
    "#username2_category = train_classification_df.set_index(\"user_id\")[\"category\"].to_dict()\n",
    "\n",
    "username2_category = train_classification_df.set_index(\"user_id\").to_dict()[\"encoded_category\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "with open(\"training-dataset.jsonl\", \"rb\") as f_in:\n",
    "    with gzip.open(\"training-dataset.jsonl.gz\", \"wb\") as f_out:\n",
    "        f_out.writelines(f_in)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_path = \"training-dataset.jsonl.gz\"\n",
    "\n",
    "username2posts_train = dict()\n",
    "username2profile_train = dict()\n",
    "\n",
    "username2posts_test = dict()\n",
    "username2profile_test = dict()\n",
    "\n",
    "\n",
    "with gzip.open(train_data_path, \"rt\") as fh:\n",
    "  for line in fh:\n",
    "    sample = json.loads(line)\n",
    "\n",
    "    profile = sample[\"profile\"]\n",
    "    username = profile[\"username\"]\n",
    "    if username in username2_category:\n",
    "      # train data info\n",
    "      username2posts_train[username] = sample[\"posts\"]\n",
    "      username2profile_train[username] = profile\n",
    "\n",
    "\n",
    "    else:\n",
    "      # it is test data info\n",
    "      username2posts_test[username] = sample[\"posts\"]\n",
    "      username2profile_test[username] = profile\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_profiles = [\n",
    "    username for username in username2posts_train.keys()\n",
    "    if username not in username2profile_train\n",
    "]\n",
    "print(\"Missing profiles:\", missing_profiles)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Profile Dataframe\n",
    "train_profile_df = pd.DataFrame(username2profile_train).T.reset_index(drop=True)\n",
    "test_profile_df = pd.DataFrame(username2profile_test).T.reset_index(drop=True)\n",
    "\n",
    "train_profile_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import re\n",
    "\n",
    "\n",
    "\n",
    "def preprocess_text(text: str):\n",
    "    # lower casing Turkish Text, Don't use str.lower :)\n",
    "    text = text.casefold()\n",
    "\n",
    "    # Remove URLs\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
    "\n",
    "    # Remove special characters and punctuation\n",
    "    # HERE THE EMOJIS stuff are being removed, you may want to keep them :D\n",
    "    text = re.sub(r'[^a-zçğıöşü0-9\\s@#]', '', text)\n",
    "\n",
    "    # Remove numbers\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "\n",
    "    # Remove extra whitespaces\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "    return text\n",
    "\n",
    "\n",
    "corpus = []\n",
    "\n",
    "# to keep the label order\n",
    "train_usernames = []\n",
    "\n",
    "for username, posts in username2posts_train.items():\n",
    "  train_usernames.append(username)\n",
    "\n",
    "  # aggregating the posts per user\n",
    "  cleaned_captions = []\n",
    "  for post in posts:\n",
    "    post_caption = post.get(\"caption\", \"\")\n",
    "    if post_caption is None:\n",
    "      continue\n",
    "\n",
    "    post_caption = preprocess_text(post_caption)\n",
    "\n",
    "    if post_caption != \"\":\n",
    "      cleaned_captions.append(post_caption)\n",
    "\n",
    "\n",
    "  # joining the posts of each user with a \\n\n",
    "  user_post_captions = \"\\n\".join(cleaned_captions)\n",
    "  corpus.append(user_post_captions)\n",
    "\n",
    "\n",
    "vectorizer = TfidfVectorizer(stop_words=turkish_stopwords, max_features=7500)\n",
    "\n",
    "\n",
    "# fit the vectorizer\n",
    "vectorizer.fit(corpus)\n",
    "\n",
    "\n",
    "# transform the data into vectors\n",
    "x_post_train = vectorizer.transform(corpus)\n",
    "y_train = [username2_category.get(uname, \"NA\") for uname in train_usernames]\n",
    "\n",
    "\n",
    "test_usernames = []\n",
    "test_corpus = []\n",
    "for username, posts in username2posts_test.items():\n",
    "  test_usernames.append(username)\n",
    "  # aggregating the posts per user\n",
    "  cleaned_captions = []\n",
    "  for post in posts:\n",
    "    post_caption = post.get(\"caption\", \"\")\n",
    "    if post_caption is None:\n",
    "      continue\n",
    "\n",
    "    post_caption = preprocess_text(post_caption)\n",
    "\n",
    "    if post_caption != \"\":\n",
    "      cleaned_captions.append(post_caption)\n",
    "\n",
    "  user_post_captions = \"\\n\".join(cleaned_captions)\n",
    "  test_corpus.append(user_post_captions)\n",
    "\n",
    "\n",
    "# Just transforming! No Fitting!!!!!\n",
    "x_post_test = vectorizer.transform(test_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "def preprocess_text(text: str):\n",
    "    # Lowercase the text (casefold handles Turkish-specific letters)\n",
    "    text = text.casefold()\n",
    "\n",
    "    # Remove URLs\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
    "\n",
    "    # Remove special characters and punctuation, but keep hashtags (#) and emojis\n",
    "    # Unicode emoji range is retained along with hashtags\n",
    "    text = re.sub(r'[^a-zçğıöşü0-9\\s#\\U0001F600-\\U0001F64F\\U0001F300-\\U0001F5FF\\U0001F680-\\U0001F6FF\\U0001F700-\\U0001F77F]', '', text)\n",
    "\n",
    "    # Remove numbers\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "\n",
    "    # Remove extra whitespaces\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "    return text\n",
    "\n",
    "corpus = []\n",
    "train_usernames = []\n",
    "\n",
    "for username, posts in username2posts_train.items():\n",
    "    train_usernames.append(username)\n",
    "\n",
    "    # Aggregating the posts, biography, and full name per user\n",
    "    cleaned_text = []\n",
    "    bio_user = username2profile_train[username].get(\"biography\", \"\") or \"\"\n",
    "\n",
    "    # Preprocess biography and prepend identifier\n",
    "    bio_user = preprocess_text(bio_user)\n",
    "\n",
    "    if bio_user != \"\":\n",
    "        cleaned_text.append(bio_user)\n",
    "\n",
    "    # Preprocess captions and prepend identifier\n",
    "    for post in posts:\n",
    "        post_caption = post.get(\"caption\", \"\")\n",
    "        if post_caption is None:\n",
    "            continue\n",
    "\n",
    "        post_caption = preprocess_text(post_caption)\n",
    "\n",
    "        if post_caption != \"\":\n",
    "            cleaned_text.append(post_caption)\n",
    "\n",
    "    # Combine all fields with a separator (e.g., \"\\n\")\n",
    "    user_text = \"\\n\".join(cleaned_text)\n",
    "    corpus.append(user_text)\n",
    "\n",
    "# Initialize TF-IDF vectorizer\n",
    "vectorizer2 = TfidfVectorizer(stop_words=turkish_stopwords, max_features=8000)\n",
    "\n",
    "# Fit the vectorizer\n",
    "vectorizer2.fit(corpus)\n",
    "\n",
    "# Transform the data into vectors\n",
    "x_train_vec = vectorizer2.transform(corpus)\n",
    "y_train = [username2_category.get(uname, \"NA\") for uname in train_usernames]\n",
    "\n",
    "# Process the test data similarly\n",
    "test_usernames = []\n",
    "test_corpus = []\n",
    "\n",
    "for username, posts in username2posts_test.items():\n",
    "    test_usernames.append(username)\n",
    "\n",
    "    cleaned_text = []\n",
    "    bio_user = username2profile_test[username].get(\"biography\", \"\") or \"\"\n",
    "\n",
    "    bio_user = preprocess_text(bio_user)\n",
    "\n",
    "    if bio_user != \"\":\n",
    "        cleaned_text.append(bio_user)\n",
    "\n",
    "    for post in posts:\n",
    "        post_caption = post.get(\"caption\", \"\")\n",
    "        if post_caption is None:\n",
    "            continue\n",
    "\n",
    "        post_caption = preprocess_text(post_caption)\n",
    "\n",
    "        if post_caption != \"\":\n",
    "            cleaned_text.append(post_caption)\n",
    "\n",
    "    # Combine all fields with a separator (e.g., \"\\n\")\n",
    "    user_text = \"\\n\".join(cleaned_text)\n",
    "    test_corpus.append(user_text)\n",
    "\n",
    "# Transform the test data (no fitting)\n",
    "x_test = vectorizer2.transform(test_corpus)\n",
    "#y_test = [username2_category.get(uname, \"NA\") for uname in test_usernames]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "def preprocess_text(text: str):\n",
    "    # Lowercase the text (casefold handles Turkish-specific letters)\n",
    "    text = text.casefold()\n",
    "\n",
    "    # Remove URLs\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
    "\n",
    "    # Remove special characters and punctuation, but keep hashtags (#) and emojis\n",
    "    # Unicode emoji range is retained along with hashtags\n",
    "    text = re.sub(r'[^a-zçğıöşü0-9\\s#\\U0001F600-\\U0001F64F\\U0001F300-\\U0001F5FF\\U0001F680-\\U0001F6FF\\U0001F700-\\U0001F77F]', '', text)\n",
    "\n",
    "    # Remove numbers\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "\n",
    "    # Remove extra whitespaces\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "    return text\n",
    "\n",
    "corpus = []\n",
    "train_usernames = []\n",
    "\n",
    "for username, posts in username2posts_train.items():\n",
    "    train_usernames.append(username)\n",
    "\n",
    "    # Aggregating the posts, biography, and full name per user\n",
    "    cleaned_text = []\n",
    "    bio_user = username2profile_train[username].get(\"biography\", \"\") or \"\"\n",
    "\n",
    "    # Preprocess biography and prepend identifier\n",
    "    bio_user = preprocess_text(bio_user)\n",
    "\n",
    "    if bio_user != \"\":\n",
    "        cleaned_text.append(bio_user)\n",
    "\n",
    "    # Preprocess captions and prepend identifier\n",
    "    for post in posts:\n",
    "        post_caption = post.get(\"caption\", \"\")\n",
    "        if post_caption is None:\n",
    "            continue\n",
    "\n",
    "        post_caption = preprocess_text(post_caption)\n",
    "\n",
    "        if post_caption != \"\":\n",
    "            cleaned_text.append(post_caption)\n",
    "\n",
    "    # Combine all fields with a separator (e.g., \"\\n\")\n",
    "    user_text = \"\\n\".join(cleaned_text)\n",
    "    corpus.append(user_text)\n",
    "\n",
    "# Initialize TF-IDF vectorizer\n",
    "vectorizer2 = TfidfVectorizer(stop_words=turkish_stopwords, max_features=7500)\n",
    "\n",
    "# Fit the vectorizer\n",
    "vectorizer2.fit(corpus)\n",
    "\n",
    "# Transform the data into vectors\n",
    "x_train_vec = vectorizer2.transform(corpus)\n",
    "y_train = [username2_category.get(uname, \"NA\") for uname in train_usernames]\n",
    "\n",
    "# Process the test data similarly\n",
    "test_usernames = []\n",
    "test_corpus = []\n",
    "\n",
    "for username, posts in username2posts_test.items():\n",
    "    test_usernames.append(username)\n",
    "\n",
    "    cleaned_text = []\n",
    "    bio_user = username2profile_test[username].get(\"biography\", \"\") or \"\"\n",
    "\n",
    "    bio_user = preprocess_text(bio_user)\n",
    "\n",
    "    if bio_user != \"\":\n",
    "        cleaned_text.append(bio_user)\n",
    "\n",
    "    for post in posts:\n",
    "        post_caption = post.get(\"caption\", \"\")\n",
    "        if post_caption is None:\n",
    "            continue\n",
    "\n",
    "        post_caption = preprocess_text(post_caption)\n",
    "\n",
    "        if post_caption != \"\":\n",
    "            cleaned_text.append(post_caption)\n",
    "\n",
    "    # Combine all fields with a separator (e.g., \"\\n\")\n",
    "    user_text = \"\\n\".join(cleaned_text)\n",
    "    test_corpus.append(user_text)\n",
    "\n",
    "# Transform the test data (no fitting)\n",
    "x_test = vectorizer2.transform(test_corpus)\n",
    "#y_test = [username2_category.get(uname, \"NA\") for uname in test_usernames]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making sure everything is fine\n",
    "assert y_train.count(\"NA\") == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = vectorizer.get_feature_names_out()\n",
    "feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tfidf = pd.DataFrame(x_train_vec.toarray(), columns=feature_names)\n",
    "df_tfidf.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tfidf = pd.DataFrame(x_post_train.toarray(), columns=feature_names)\n",
    "df_tfidf.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "def get_avg_like_count(username):\n",
    "    def calculate_avg(posts: list):\n",
    "        total = 0\n",
    "        count = 0\n",
    "        for post in posts:\n",
    "            like_count = post.get(\"like_count\", 0)\n",
    "            if like_count is None:\n",
    "                like_count = 0\n",
    "            total += like_count\n",
    "            count += 1\n",
    "\n",
    "        return total / count if count > 0 else 0\n",
    "\n",
    "    # Check if the username exists in training or testing data\n",
    "    if username in username2posts_train:\n",
    "        return calculate_avg(username2posts_train[username])\n",
    "    elif username in username2posts_test:\n",
    "        return calculate_avg(username2posts_test[username])\n",
    "    else:\n",
    "        print(f\"No data available for {username}\")\n",
    "        return -1\n",
    "\n",
    "# Initialize an empty list to store rows\n",
    "data = []\n",
    "# Loop through usernames in the training set\n",
    "for username in train_usernames:\n",
    "    # Get profile info for the user\n",
    "    profile = username2profile_train[username]\n",
    "\n",
    "    average_like = get_avg_like_count(username)\n",
    "    # Extract profile-related features\n",
    "    profile_info = {\n",
    "        #\"username\": username,\n",
    "        \"follower_count\": profile.get(\"follower_count\", 0) or 0,\n",
    "        \"following_count\": profile.get(\"following_count\", 0) or 0,\n",
    "        #\"highlight_reel_count\": profile.get(\"highlight_reel_count\", 0) or 0,\n",
    "        #\"post_count\": profile.get(\"post_count\", 0) or 0,\n",
    "        \"is_verified\": profile.get(\"is_verified\", False),\n",
    "        \"is_business_account\": profile.get(\"is_business_account\", False),\n",
    "        \"is_private\": profile.get(\"is_private\", False),\n",
    "        #\"category_name\": profile.get(\"category_name\", \"Unknown\"),\n",
    "        \"average_like\": average_like\n",
    "    }\n",
    "    #bio = profile.get(\"biography\", \"\") or \"\"\n",
    "    #if bio:\n",
    "    #    bio = preprocess_text(bio)\n",
    "    #    if bio:  # Ensure the processed caption is not empty\n",
    "    #        bio_corpus.append(bio)\n",
    "\n",
    "    data.append(profile_info)\n",
    "\n",
    "# Convert the list of dictionaries into a DataFrame\n",
    "df_train_pro = pd.DataFrame(data)\n",
    "\n",
    "# label encoder\n",
    "#label_encoder = LabelEncoder()\n",
    "#df_train_pro[\"category_name\"] = label_encoder.fit_transform(df_train_pro['category_name'])\n",
    "\n",
    "# Fit and transform the TF-IDF vectorizer\n",
    "#vectorizer_bio = TfidfVectorizer(stop_words=turkish_stopwords, max_features=500)\n",
    "#x_bio_tfidf = vectorizer_bio.fit_transform(bio_corpus)\n",
    "#tfidf_df_bio = pd.DataFrame(x_bio_tfidf.toarray(), columns=vectorizer_bio.get_feature_names_out())\n",
    "#print(f\"TF-IDF matrix shape: {x_bio_tfidf.shape}\")\n",
    "\n",
    "# Convert binary columns (True/False) to 0/1\n",
    "binary_columns = ['is_verified', 'is_private', 'is_business_account']\n",
    "for col in binary_columns:\n",
    "    df_train_pro[col] = df_train_pro[col].astype(int)\n",
    "\n",
    "df_combined = pd.concat([df_train_pro.reset_index(drop=True), df_tfidf.reset_index(drop=True)], axis=1)\n",
    "df_combined.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the correlation matrix\n",
    "correlation_matrix = df_train_pro.corr()\n",
    "\n",
    "# Print the correlation matrix\n",
    "correlation_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Step 1: Standardize the data\n",
    "scaler = StandardScaler()\n",
    "scaled_data = scaler.fit_transform(df_train_pro)\n",
    "\n",
    "# Step 2: Apply PCA\n",
    "pca = PCA(n_components=2)  # Reduce to 2 components (adjust as needed)\n",
    "principal_components = pca.fit_transform(scaled_data)\n",
    "\n",
    "# Step 3: Create a DataFrame with principal components\n",
    "pca_df = pd.DataFrame(data=principal_components, columns=['PC1', 'PC2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pca_tfidf = pd.concat([pca_df.reset_index(drop=True), df_tfidf.reset_index(drop=True)], axis=1)\n",
    "df_pca_tfidf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set the number of components to a valid value\n",
    "n_components = min(3000, 500)  # Use the smaller of 3000 features or 500 components\n",
    "svd = TruncatedSVD(n_components=n_components, random_state=42)\n",
    "\n",
    "# Fit the Truncated SVD\n",
    "svd.fit(df_tfidf)\n",
    "\n",
    "# Plot cumulative explained variance\n",
    "cumulative_variance = svd.explained_variance_ratio_.cumsum()\n",
    "plt.plot(range(1, len(cumulative_variance) + 1), cumulative_variance, marker='o')\n",
    "plt.xlabel('Number of Components')\n",
    "plt.ylabel('Cumulative Explained Variance')\n",
    "plt.title('PCA Components vs. Explained Variance')\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Apply Truncated SVD\n",
    "svd = TruncatedSVD(n_components=1000, random_state=42)  # Reduce to 1000 components\n",
    "reduced_matrix = svd.fit_transform(df_tfidf)\n",
    "# Convert to DataFrame\n",
    "columns = [f'PC_TFIDF{i+1}' for i in range(reduced_matrix.shape[1])]  # Create column names like PC1, PC2, ...\n",
    "reduced_df = pd.DataFrame(reduced_matrix, columns=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pca_tfidf2 = pd.concat([pca_df.reset_index(drop=True), reduced_df.reset_index(drop=True)], axis=1)\n",
    "df_pca_tfidf2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_combined.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X = df_tfidf\n",
    "y = y_train\n",
    "x_train, x_val, y_train, y_val = train_test_split(df_tfidf, y_train, test_size=0.2, stratify=y_train, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "#NEW NEW BESTTT\n",
    "from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier, StackingClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Define Stacking Classifier\n",
    "stacking_model2 = StackingClassifier(\n",
    "    estimators=[\n",
    "        ('gb', GradientBoostingClassifier(learning_rate=0.1, max_depth=7, max_features='sqrt', min_samples_leaf=4, min_samples_split=2, n_estimators=200, subsample=0.8, random_state=42)),\n",
    "        ('lr_l1', LogisticRegression(penalty='l1', solver='liblinear', max_iter=1000, random_state=42)),\n",
    "        ('lr_l2', LogisticRegression(penalty='l2', solver='lbfgs', max_iter=1000, random_state=42)),\n",
    "        ('svm', SVC(C=50, class_weight='balanced', degree=2, gamma=0.01, kernel='rbf', random_state=42) ),\n",
    "        ('mlp', MLPClassifier(activation='tanh', alpha=0.001, hidden_layer_sizes=(100, 100), learning_rate='adaptive', learning_rate_init=0.001, solver='adam', random_state=42)) # Neural Network\n",
    "    ],\n",
    "    final_estimator=RandomForestClassifier(n_estimators=100, random_state=42, bootstrap=False, criterion='gini', max_depth=50, min_samples_leaf=2, min_samples_split=10)\n",
    ")\n",
    "\n",
    "# Use cross-validation to get a stable performance estimate\n",
    "scores = cross_val_score(stacking_model2, X, y, cv=5)\n",
    "print(f\"Mean Accuracy: {scores.mean():.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Test Data\n",
    "\n",
    "\n",
    "# let's take a look at the first 5 lines of the file\n",
    "test_data_path = \"test-classification-round2.dat\"\n",
    "!head -n 5 \"$test_data_path\"\n",
    "\n",
    "print(\"*****\")\n",
    "\n",
    "test_unames = []\n",
    "with open(test_data_path, \"rt\") as fh:\n",
    "  for line in fh:\n",
    "    test_unames.append(line.strip())\n",
    "\n",
    "print(test_unames[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = []\n",
    "\n",
    "for uname in test_unames:\n",
    "  try:\n",
    "    index = test_usernames.index(uname)\n",
    "    x_test.append(x_post_test[index].toarray()[0])\n",
    "  except Exception as e:\n",
    "    try:\n",
    "      index = train_usernames.index(uname)\n",
    "      x_test.append(x_post_train[index].toarray()[0])\n",
    "    except Exception as e:\n",
    "      print(uname)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.DataFrame(np.array(x_test), columns=feature_names)\n",
    "df_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred = stacking_model.predict(df_test)\n",
    "original_categories = label_encoder.inverse_transform(test_pred)\n",
    "\n",
    "output = dict()\n",
    "for index, uname in enumerate(test_unames):\n",
    "  output[uname] = original_categories[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Length of test_unames:\", len(test_unames))\n",
    "print(\"Length of test_pred:\", len(test_pred))\n",
    "print(\"Length of original_categories:\", len(original_categories))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"prediction-classification-round2.json\", \"w\") as of:\n",
    "  json.dump(output, of, indent=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
